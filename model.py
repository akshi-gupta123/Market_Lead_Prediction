# -*- coding: utf-8 -*-
"""Mini_Project_6_sem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsRJASjcL1hjLdoj3qBwLZR8zhKmhwrE
"""


"""#Importing Libraries"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as ms

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score
from sklearn.metrics import precision_recall_curve

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score


from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import BernoulliNB, GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier

"""# Importing Data"""

df=pd.read_csv('/content/Marketing_Leads_India.csv.gz')
pd.set_option('display.max_columns',None)

df.head()

"""Checking the DataFrame"""

df.info()

plt.title("Null Values Plot",fontsize=50)
ms.bar(df)

df.shape

"""Calculating percentage of null  values in data"""

round(df.isnull().sum()*100/df.shape[0],2 )

"""Dropping the columns which have more than 70% of null values """

drop_col = df.loc[:,list(round(df.isnull().sum()*100/df.shape[0],2)>70.00)].columns
df=df.drop(drop_col,1)

df.head()

df.shape

"""## `Exploratory Data Analysis`"""

#Calculating the percentage of unique values present in a column
for col in df.select_dtypes(include = 'object').columns:
  print(col)
  print("********************************************************************\n")
  print(df[col].value_counts(normalize=True)*100)
  print("********************************************************************\n")

"""# Data Preparation"""

#Finding list of columns which have categorical values (Yes/No)
discrete_col = []
for col in  df:
  if 'Yes' in df[col].unique() or 'No' in df[col].unique():
    discrete_col.append(col)
discrete_col

"""Replacing the Yes to 1 and No to 0"""

for feature in discrete_col:
  df[feature].replace(('Yes','No'),(1,0),inplace = True)
df.head()

"""**Handling the missing values**

A COMMON WAY TO REPLACE EMPTY CELLS, IS TO CALCULATE THE MEAN,MEDIAN OR MODE VALUE OF THE COLUMN.A COMMON WAY TO REPLACE EMPTY CELLS, IS TO CALCULATE THE MEAN,MEDIAN OR MODE VALUE OF THE COLUMN

Here, we would replace the value that appears most frequently.(**MODE**)
"""

null_col = ['Lead Quality','City','Specialization','Tags','What matters most to you in choosing a course','What is your current occupation','Country']
for col in null_col:
  x= df[col].mode()[0]
  df[col].fillna(x, inplace = True)

#Finding the percentage of null values present in each column
round(df.isnull().sum()*100/df.shape[0],2 )

"""Plotting null Values"""

plt.figure(figsize=(15,13))
plt.subplot(321)
plt.title("Asymmetrique Profile Score")
plt.plot(df['Asymmetrique Profile Score'],color='r')
plt.subplot(322)
plt.title("Asymmetrique Activity Score")
plt.plot(df['Asymmetrique Activity Score'],color='r')
plt.subplot(323)
plt.title("Asymmetrique Profile Score After ffill")
plt.plot(df['Asymmetrique Profile Score'].fillna(method = 'ffill'),color='m')
plt.subplot(324)
plt.title("Asymmetrique Activity Score After ffill")
plt.plot(df['Asymmetrique Activity Score'].fillna(method = 'ffill'),color='m')
plt.subplot(325)
plt.title("Asymmetrique Profile Score After Linear Interpolation")
plt.plot(df['Asymmetrique Profile Score'].interpolate(),color='g')
plt.subplot(326)
plt.title("Asymmetrique Activity Score After Linear Interpolation")
plt.plot(df['Asymmetrique Activity Score'].interpolate(),color='g')
plt.tight_layout()

"""Here, 'Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score' have large percentage of null values. So we should drop these columns."""

df = df.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score'],1)

"""Now the remaining columns have less than 2% null values. So we can drop these rows directly."""

df.dropna(inplace = True)
df.head()

"""We can do further analysis with good representation of both the classes."""

sns.countplot(data=df,x='Lead Source',hue='Converted')
plt.xticks(rotation=90)
plt.show()

"""We can see there are many values which are looking negligible with compare to other values.So, we should remove them."""

df['Lead Source'].value_counts()

df['Lead Source'] = df['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',
  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Other_Lead_Source')
df.head()

#google and Google are same but are represented as different columns.
#So we should replace all occurrences of one column into another column.
df['Lead Source'] = df['Lead Source'].replace('google','Google')

#Representation of Lead Source when unwanted values are removed

sns.countplot(data=df,x='Lead Source',hue='Converted')
plt.xticks(fontsize=13,rotation=90)

plt.xlabel("Lead Source",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted')
plt.show()

"""The lead count are maximum in Direct Traffic and Google.
Conversion rate is maximum in Reference and Welingak Website.
"""

sns.countplot(data=df,x='Lead Origin',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Lead Origin",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=15, title_fontsize='15',title='Converted')
plt.show()

"""API and Landing Page Submission have almost 35% conversion rate and Lead Add Form has a high conversion rate. And Lead import has a very low lead count and conversion rate.

"""

sns.countplot(data=df,x='Last Activity',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Last Activity",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

# Converting all the low count categories to the 'Others' category
df['Last Activity'] = df['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked', 
                                                       'Visited Booth in Tradeshow', 'Approached upfront',
                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'], 'Other Activity')

# lets plot the Last Activity again 
sns.countplot(data=df,x='Last Activity',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Last Activity",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

"""The counts of Email opened as Last Activity and the conversion rate of SMS sent is maximum.

"""

sns.countplot(data=df,x='Tags',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Tags",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

"""We should convert the tags which have very less count as Other_Tags"""

df['Tags'] = df['Tags'].replace(['In confusion whether part time or DLP', 'in touch with EINS','Diploma holder (Not Eligible)',
                                     'Approached upfront','Graduation in progress','number not provided', 'opp hangup','Still Thinking',
                                    'Lost to Others','Shall take in the next coming month','Lateral student','Interested in Next batch',
                                    'Recognition issue (DEC approval)','Want to take admission but has financial problems',
                                    'University not recognized'], 'Other_Tags')
sns.countplot(data=df,x='Tags',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Tags",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

"""Will revert after reading the email has very high count and conversion rate. And Closed by horizon also has high conversion rate."""

sns.countplot(data=df,x='Lead Quality',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Lead Quality",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

sns.countplot(data=df,x='Specialization',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("Specialization",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

"""Visualizing the plot of Specialization, no specific inference can be made for it."""

sns.countplot(data=df,x='What is your current occupation',hue='Converted')
plt.xticks(fontsize=14,rotation=90)
plt.xlabel("What is your current occupation",fontsize=17)
plt.ylabel("Count", fontsize=17)
plt.legend(fontsize=10, title_fontsize='15',title='Converted',loc='upper right')
plt.show()

"""Visualizing the graph, Working Professional has a high conversion rate. And Unemployed has a huge lead count."""

df.columns

# Dropping unnecessary columns which have only single unique values and they don't have any impact on target variable
df = df.drop(df.columns[[0,4,10,13,14,15,16,17,18,19,20,21,24,25,27,28]],axis=1)
df.head

cat_col = df.dtypes[~(df.dtypes == 'int64') & ~(df.dtypes == 'float64')].keys()
cat_col

# get dummy variables of categorical columns to convert the string into integer
dummy = pd.get_dummies(df[cat_col],drop_first=True)
dummy.head()

df = df.drop(cat_col,axis=1)
df.head()

df = pd.concat([df,dummy],axis=1)
df.head()

# Separating target variable and input variables 
x = df.drop('Converted',axis=1)
y = df['Converted']
x.head()

#Split the data into train and test
x_train, x_test, y_train,y_test=train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=42)

print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)

"""## Feature Scaling"""

scaler=StandardScaler()
x_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']]=scaler.fit_transform(x_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])

x_train.head()

# setting the figure size

plt.figure(figsize=(25,15))

# setting the title

plt.title('Correlations',fontsize=50)

# Plotting a heatmap

sns.heatmap(df.corr(method='spearman'))

plt.show()

cor_matrix = df.corr().abs()
print(cor_matrix)

upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))
print(upper_tri)

to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]
print(); print(to_drop)

x_train = x_train.drop(to_drop, axis=1)
x_test = x_test.drop(to_drop, axis=1)
print(x_train.shape,x_test.shape)

ct = ColumnTransformer([('se', StandardScaler(), ['Total Time Spent on Website', 'Page Views Per Visit', 'TotalVisits'])], remainder='passthrough')

random_forest_pipeline = Pipeline([('transformer', ct), ('RandomForest', RandomForestClassifier(random_state = 42))])
adaboost_pipeline = Pipeline([('transformer', ct), ('Adaboost', AdaBoostClassifier(random_state = 42))])
ExtraTree_pipeline = Pipeline([('transformer', ct), ('ExtraTreeClassifier', ExtraTreesClassifier(random_state = 42))])
BaggingClassifier_pipeline = Pipeline([('transformer', ct), ('BaggingClassifier', BaggingClassifier(base_estimator = DecisionTreeClassifier(), random_state = 42))])
GradientBoost_pipeline = Pipeline([('transformer', ct), ('GradientBoosting', GradientBoostingClassifier(random_state = 42))])
dtree_pipeline = Pipeline([('transformer', ct), ('DecisionTree', DecisionTreeClassifier(random_state = 42))])
knn_pipeline = Pipeline([('transformer', ct), ('KNN', KNeighborsClassifier())])
lr_pipeline = Pipeline([('transformer', ct), ('LogisticRegression', LogisticRegression(random_state = 42,solver='lbfgs', max_iter=100))])
sgd_pipeline = Pipeline([('transformer', ct), ('StochasticGradient', SGDClassifier(random_state = 42))])
mlp_pipeline = Pipeline([('transformer', ct), ('MLPClassifier', MLPClassifier(random_state = 42))])
naive_pipeline = Pipeline([('transformer', ct), ('NaiveBayes', GaussianNB())])
svc_pipeline = Pipeline([('transformer', ct), ('SVM', SVC(random_state = 42))])
lightgbm_pipeline = Pipeline([('transformer', ct), ('lightgbm', LGBMClassifier(random_state = 42))])
catboost_pipeline = Pipeline([('transformer', ct), ('CatBoost', CatBoostClassifier(random_state = 42, silent = True))])
xgboost_pipeline = Pipeline([('transformer', ct), ('XGBoost', XGBClassifier(random_state = 42))])

pipeline_list = [random_forest_pipeline, adaboost_pipeline, ExtraTree_pipeline, BaggingClassifier_pipeline, GradientBoost_pipeline,
                dtree_pipeline, knn_pipeline, lr_pipeline, sgd_pipeline, mlp_pipeline, naive_pipeline, svc_pipeline,
                lightgbm_pipeline, catboost_pipeline, xgboost_pipeline]

pipe_dict = {0: "RandomForest", 1: "Adaboost", 2: "ExtraTree", 3: "BaggingClassifier", 4: "GradientBoosting", 5: "DecisionTree",
            6: "KNN", 7: "Logistic", 8: "SGD Classifier", 9: "MLPClassifier", 10: "NaiveBayes",
            11: "SVM", 12: "LightGBM", 13: "Catboost", 14: "XGBoost"}

for idx, pipe in enumerate(pipeline_list):
    score = cross_val_score(pipe, x_train, y_train, cv = 10, scoring = 'accuracy')
    print(pipe_dict[idx], ":", score.mean())

"""We will choose top 5 algorithms on the basis of these cross-validation scores.
XGBoost : 0.9283
GradientBoosting : 0.9276
Catboost : 0.9271
LightGBM : 0.9260
SVM : 0.9247


# XGBoost Classifier

XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.
"""

xgb=XGBClassifier(random_state=42)

model = xgb.fit(x_train, y_train)
predict_train_y = model.predict(x_train)
predict_test_y = model.predict(x_test)
xgb_test_score = metrics.accuracy_score(predict_test_y,y_test)
xgb_train_score = metrics.accuracy_score(predict_train_y,y_train)
print("Testing Accuracy : ", xgb_test_score)
print("Training Accuracy : ", xgb_train_score)

test_cf_report = pd.DataFrame(classification_report(y_test, predict_test_y, output_dict = True))
print(test_cf_report)

test_conf = confusion_matrix(y_test, predict_test_y)
sns.heatmap(test_conf, square=True, annot=True, cmap='YlGnBu', fmt='g', cbar=False)
plt.title('Confusion matrix')
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')

"""# Gradient Boosting Classifier

The gradient is used to minimize the loss function (error - difference between the actual values and predicted values). It is basically the partial derivative of the loss function, so it describes the steepness of our error function.
"""

gB=GradientBoostingClassifier(random_state=42)

model = gB.fit(x_train, y_train)
predict_train_y = model.predict(x_train)
predict_test_y = model.predict(x_test)
gradientBoost_test_score = metrics.accuracy_score(predict_test_y,y_test)
gradientBoost_train_score = metrics.accuracy_score(predict_train_y,y_train)
print("Testing Accuracy : ", gradientBoost_test_score)
print("Training Accuracy : ", gradientBoost_train_score)

test_gb_report = pd.DataFrame(classification_report(y_test, predict_test_y, output_dict = True))
print(test_gb_report)

test_conf = confusion_matrix(y_test, predict_test_y)
sns.heatmap(test_conf, square=True, annot=True, cmap='YlGnBu', fmt='g', cbar=False)
plt.title('Confusion matrix')
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')

"""# Catboost Classifier

CatBoost is a fast, scalable, high performance gradient boosting on decision trees library. Used for ranking, classification, regression and other ML tasks.

"""

catboost_classif = CatBoostClassifier(random_state=42, silent = True)
model = catboost_classif.fit(x_train, y_train)
predict_train_y = model.predict(x_train)
predict_test_y = model.predict(x_test)
catBoost_test_score = metrics.accuracy_score(predict_test_y,y_test)
catBoost_train_score = metrics.accuracy_score(predict_train_y,y_train)
print("Testing Accuracy : ", catBoost_test_score)
print("Training Accuracy : ", catBoost_train_score)

test_cB_report = pd.DataFrame(classification_report(y_test, predict_test_y, output_dict = True))
print("Classification Report : ")
print(test_cB_report)

test_conf = confusion_matrix(y_test, predict_test_y)
sns.heatmap(test_conf, square=True, annot=True, cmap='YlGnBu', fmt='g', cbar=False)
plt.title('Confusion matrix')
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')

"""# LGBM Classifier

Light Gradient Boosting Machine is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:

Faster training speed and higher efficiency.
Lower memory usage.
Better accuracy.
Support of parallel, distributed, and GPU learning.
Capable of handling large-scale data.
"""

lgbm = LGBMClassifier(random_state=42)
model = lgbm.fit(x_train, y_train)
predict_train_y = model.predict(x_train)
predict_test_y = model.predict(x_test)
lgbm_test_score = metrics.accuracy_score(predict_test_y,y_test)
lgbm_train_score = metrics.accuracy_score(predict_train_y,y_train)
print("Testing Accuracy : ", lgbm_test_score)
print("Training Accuracy : ", lgbm_train_score)

test_lgbm_report = pd.DataFrame(classification_report(y_test, predict_test_y, output_dict = True))
print("Classification Report :")
print(test_lgbm_report)

test_conf = confusion_matrix(y_test, predict_test_y)
sns.heatmap(test_conf, square=True, annot=True, cmap='YlGnBu', fmt='g', cbar=False)
plt.title('Confusion matrix')
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')

"""# Support Vector Machine

Support Vector Machine(SVM) is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well its best suited for classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points.
"""

svm_classifier = SVC(random_state=42)

model = svm_classifier.fit(x_train, y_train)
predict_train_y = model.predict(x_train)
predict_test_y = model.predict(x_test)
svm_classifier_test_score = metrics.accuracy_score(predict_test_y,y_test)
svm_classifier_train_score = metrics.accuracy_score(predict_train_y,y_train)
print("Testing Accuracy : ", svm_classifier_test_score)
print("Training Accuracy : ", svm_classifier_train_score)

test_svm_classifier_report = pd.DataFrame(classification_report(y_test, predict_test_y, output_dict = True))
print(test_svm_classifier_report)

test_conf = confusion_matrix(y_test, predict_test_y)
sns.heatmap(test_conf, square=True, annot=True, cmap='YlGnBu', fmt='g', cbar=False)
plt.title('Confusion matrix')
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')

acc_score=pd.DataFrame(columns=["model", "train_accuracy","test_accuracy"])
acc_score

acc_score["model"]=["LGBM Classifier","CatBoost Classifier","XG Boost Classifier", "Gradient Boosting Classifier","SVM Classifier"]
acc_score["train_accuracy"]=[lgbm_train_score,catBoost_train_score,xgb_train_score,gradientBoost_train_score , svm_classifier_train_score]
acc_score["test_accuracy"]=[lgbm_test_score,catBoost_test_score,xgb_test_score,gradientBoost_test_score , svm_classifier_test_score]
acc_score

"""From the above dataframe, we can see Gradient Boosting Classifier is performing much better than other models. """

